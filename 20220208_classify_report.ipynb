{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-WordsとLightGBMで恋愛コラムの本文からライターを判定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    "\n",
    "- [Googirl](https://googirl.jp/)という女性向け恋愛コラムのWebサイトがあります。\n",
    "- 記事ごとに書いているライターが異なります。本文をMeCabで形態素解析して作ったBag-of-Words (BoW) を特徴量としてライターを分類するLightGBMモデルを作ってみました。\n",
    "  - コラムの総数が多い上位5人のライターに絞っています。記事数は4000件程度、1本の記事は約500文字程度です。\n",
    "  - 記事のBoWを特徴量にした5クラス分類タスクです。\n",
    "  - LightGBMはoptuna.integration.lightgbmを使いました。\n",
    "- 予測精度は95%程度でした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ\n",
    "\n",
    "- 対象は、Googirlの[恋愛・結婚カテゴリ](https://googirl.jp/renai/)のうち、2017/1/1～2021/6/15に書かれたコラムです。そのうち、投稿総数上位5位のライターのみを取り出しました。\n",
    "  - ただし、続き物のコラムは最初の1本のみを取り出しました。\n",
    "    - vol.1～vol.9や第1位～第9位、前編・後編のように、同じライターが同じテーマを複数の記事に分割していることが多いです。この場合、vol.1、第1位、前編のみをデータに残し、それ以外の記事は含めていないということです。というのも、これらの記事は同じテーマであるため、ある意味ライターを予測できるのが当たり前だからです。\n",
    "  - 全部で4302件の記事です。\n",
    "- Googirlの恋愛・結婚カテゴリをスクレイピングし、タイトル（使わない）、本文、ライター名を列に持つpd.DataFrameにしました。\n",
    "  - 他にもカテゴリはありますが、同じカテゴリの文章を分類する方が難易度が高く面白いと思ったので、恋愛・結婚カテゴリに絞りました。\n",
    "  - 十分なスリープを挟みました。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    author     n\n",
      "0     松はるな  1544\n",
      "1        和   729\n",
      "2     小林ユリ   682\n",
      "3     Waxy   680\n",
      "4       美佳   667\n",
      "..     ...   ...\n",
      "78   メカセツコ     1\n",
      "79  羅生門の老婆     1\n",
      "80  きゃおりーな     1\n",
      "81     イッヌ     1\n",
      "82    Risa     1\n",
      "\n",
      "[83 rows x 2 columns]\n",
      "['松はるな', '和', '小林ユリ', 'Waxy', '美佳']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import neologdn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import optuna.integration.lightgbm as lgb\n",
    "from optuna.logging import set_verbosity\n",
    "from lightgbm import Dataset, early_stopping, log_evaluation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from customized_tagger import CustomizedTagger\n",
    "\n",
    "# 読み込みと前処理 ---------------------------------------------------------------------\n",
    "freq_author=5\n",
    "googirl_article=pd.read_csv(\"../data/googirl/googirl_article_renai_and_marriage_20210615.csv\")\n",
    "googirl_article=(\n",
    "    googirl_article\n",
    "    .assign(date=lambda d: pd.to_datetime(d.date))\n",
    "    .loc[lambda d: d.date>=datetime(2017,1,1)]\n",
    "    .assign(\n",
    "        title=lambda d: d.title.map(neologdn.normalize),\n",
    "    ).loc[\n",
    "        # [vV]ol. 1や前編、第1位以外の連番のタイトルの記事を削除する\n",
    "        (lambda d: \n",
    "            ~d.title.str.contains(r\"[vV]ol\\. ?[2-9]\") &\n",
    "            ~d.title.str.contains(\"[中後]編\") &\n",
    "            ~d.title.str.contains(\"第[2-9]位\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ライター別の記事数\n",
    "n_by_author=(\n",
    "    googirl_article.author\n",
    "    .value_counts(ascending=False)\n",
    "    # indexにあるauthorを列として戻すため\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\":\"author\",\"author\":\"n\"})\n",
    ")\n",
    "print(n_by_author)\n",
    "used_authors=n_by_author.author.head(freq_author).to_list()\n",
    "print(used_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "\n",
    "- 以下を削除しました。\n",
    "  - htmlタグ\n",
    "  - 下三角形やかぎかっこの前の中黒\n",
    "    - ディバイダー的に使われているものです。\n",
    "  - 記事内に出てくるインタビューした男性の年齢や職種が（）内に書いてあります。これを削除しました。\n",
    "    - 削除しないとBoWを作る上で結構なノイズになることが後で分かりました。\n",
    "- 正規化はneologd.normalizeを用いました。\n",
    "  - NFKC正規化にいくつかの表記の揺れを直してくれる便利なライブラリです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  id     id_url author                             title  \\\n",
      "0  0  210614009     美佳              12星座別!彼の究極の欠点とは?【前編】   \n",
      "1  1  210614006   松はるな    何が違うの?「彼女にしたい女」と「友達止まりの女」の3つの差   \n",
      "2  2  210614019   Waxy  直感って侮れない!「この人と結婚するかも…」が当たったケース4つ   \n",
      "3  3  210613006   松はるな     運命かも!男性が「相性のよさ」を感じて惚れる女性の言動3つ   \n",
      "4  4  210613015   Waxy   幼稚で付き合いきれない!「ナルシストだな…」と思う彼の言動4つ   \n",
      "\n",
      "                                                text  \n",
      "0  誰にでも一つくらいは欠点があるもの。付き合う上では、相手の欠点を受け入れられるかどうかがかな...  \n",
      "1  いい感じに仲よくなってきたな……と思いきや、「俺たちっていい友達だよな!」とまさかの友達発言...  \n",
      "2  長いあいだ婚活を続けてもなかなかうまくいかない人もいれば、短期間でサクッと結婚を決めるような...  \n",
      "3  「この人とは相性がいいかも!」と感じる相手とは関係が発展しやすいし、付き合うことになっても特...  \n",
      "4  見た目は一人前の立派な大人の男性。でも中身は幼稚で、いつも周りから注目されていないと不満……...  \n"
     ]
    }
   ],
   "source": [
    "article=(\n",
    "    googirl_article\n",
    "    .loc[lambda d: d.author.isin(used_authors)]\n",
    "    # urlの最後のサブディレクトリ（https://hoge.com/fuga/piyo/のpiyo）を取り出して一意なキーとして使う\n",
    "    .assign(id_url=lambda d: d.url.str.extract(\"([^/]+)(?=/$)\",expand=True))\n",
    "    # 連番を振る\n",
    "    .assign(id=lambda d: pd.RangeIndex(start=0,stop=len(d),step=1).astype(str))\n",
    "    # .assign(text=lambda d: d.title)\n",
    "    .assign(\n",
    "        text=lambda d: d.text.str.replace(r\"[\\(（][0-9]+.*[）\\)]\",\"\",regex=True)\n",
    "                                .str.replace(r\"\\[\\[\\[p\\]\\]\\]\",\"\",regex=True)\n",
    "                                .str.replace(\"▽\",\"\",regex=True)\n",
    "                                .str.replace(\"・(?=「)\",\"\",regex=True)\n",
    "                                .map(neologdn.normalize)\n",
    "     )\n",
    "     .reset_index()\n",
    "     .filter([\"id\",\"id_url\",\"url\",\"date\",\"title\",\"author\",\"text\"])\n",
    ")\n",
    "\n",
    "df=article.filter([\"id\",\"id_url\",\"author\",\"title\",\"text\"])\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析と特徴量のBoW作成\n",
    "\n",
    "- Windows 10 + MeCab 32bit (v0.996) + [mecab](https://pypi.org/project/mecab/) (v.0.996.3) を用いました。\n",
    "  - 辞書はデフォルトのIPA辞書と2020/8/20（本記事を執筆時で最新）のmecab-ipadic-NEologdです。\n",
    "  - MeCabのPythonバインディングは、私の環境ではmecab-python3のインストールに失敗したのでmecabを用いています。\n",
    "- 特徴量\n",
    "  - 名詞のみ用いています。\n",
    "  - ストップワードに[Slothlib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)を用いています。\n",
    "  - BoWは0か1かの2値にしています。\n",
    "  - scikit-learnのCountVectorizerのmin_dfを0.001にしています。\n",
    "    - つまり、4302記事の中の4記事以下でしか出現しない単語を削除しています。\n",
    "    - min_dfを0にすると13136個の特徴量（＝13136次元）ですが、0.001にしたことで4568次元まで減りました。\n",
    "- CountVectorizerには、原型のまま分かち書きをして名詞のみ残したものを渡しています。\n",
    "  - 実装のコツとして、MeCab.Tagger(\"-Ochasen\");MeCab.Tagger.parseToNodeの結果をpd.DataFrameで返すラッパーの自作クラスを作っています（末尾のセルを参照）。このDataFrameから品詞が名詞のもののみを抽出して、その原型の空白を挟んでくっつけています。\n",
    "- 余談ですが私はRとPythonの両方でMeCabを用いています。Windows環境でのMeCabは、RではShift-JIS、PythonではUTF-8の辞書を用いるため、デフォルトのipadicとNEologdともにShift-JISとUTF-8の両方の辞書を作成しています。MeCab.Tagger()は、MeCab.Tagger(\"-r .mecabrcのパス\")で使用する.mecabrcを指定できますので、UTF-8版の辞書を記載した.mecabrcを引数に与えることでUTF-8の辞書を使えるようにしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4302/4302 [00:07<00:00, 548.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   100  100年  10代  10分  10年  12月  180度  1か月  1人  1回\n",
      "0    0     0    0    0    0    0     0    0   0   0\n",
      "1    0     0    0    0    0    0     0    0   0   0\n",
      "2    0     0    0    0    0    0     0    0   0   0\n",
      "3    0     0    0    0    0    0     0    0   0   0\n",
      "4    0     0    0    0    0    0     0    0   0   0\n"
     ]
    }
   ],
   "source": [
    "# 形態素解析と特徴量を作る ---------------------------------------------------------------------\n",
    "ct=CustomizedTagger(\"-Ochasen -r ../utf8.mecabrc\")\n",
    "texts=df.text.to_list()\n",
    "# 形態素解析し、名詞のみを取り出して空白区切りの分かち書きにする\n",
    "# [np.array([\"吾輩 猫 だ\"],np.array([\"名前 まだ ない\"])]のようなもの\n",
    "wakati=[np.array(ct.wakati_extract(j,[\"名詞\"])) for i,j in enumerate(tqdm(texts))]\n",
    "# np.array([\"吾輩 猫\",\"名前 まだ ない\"])にする\n",
    "wakati=np.concatenate(wakati)\n",
    "\n",
    "# Bag-of-Wordを作る\n",
    "stop_words=(\n",
    "    pd.read_csv(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\",header=None)\n",
    "    .set_axis([\"word\"],axis=1)\n",
    "    .word\n",
    "    .to_list()\n",
    ")\n",
    "cv=CountVectorizer(min_df=0.001,binary=True,stop_words=stop_words)\n",
    "cv_transform=cv.fit_transform(wakati)\n",
    "df_x=pd.DataFrame(cv_transform.toarray(),columns=cv.get_feature_names_out())\n",
    "x=np.array(df_x)\n",
    "\n",
    "# 目的変数のauthor列をラベルエンコーティングする\n",
    "le=LabelEncoder()\n",
    "df_y=df.assign(target=lambda d: le.fit_transform(d.author))\n",
    "y=np.array(df_y.target)\n",
    "\n",
    "print(df_x.iloc[:,0:10].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習と評価\n",
    "\n",
    "- Nested CVを用いています。\n",
    "  - 外側のCV（学習と予測のFold）と内側のCV（学習の中のハイパーパラメータの調整のFold）のいずれも、5-FoldのStratified Foldとしています。\n",
    "    - Stratifiedなのはラベルが不均衡だからです。\n",
    "  - optunaのlightGBM実装を用いました。ハイパーパラメータ探索はoptunaのベイズ最適化です。\n",
    "  - i9-9900Kの環境で1～2時間で終わっていたと思います。メモリは4GB程度使っていました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習と予測 ---------------------------------------------------------------------\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': freq_author,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'deterministic': True,\n",
    "    'force_row_wise': True\n",
    "    }\n",
    "set_verbosity(1)\n",
    "callbacks=[\n",
    "    early_stopping(stopping_rounds=20),\n",
    "    log_evaluation(period=20)\n",
    "]\n",
    "\n",
    "start_time=time.time()\n",
    "pred_all=[]\n",
    "true_all=[]\n",
    "test_idx_all=[]\n",
    "# 外側のCV（学習データと評価データ）\n",
    "fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1234)\n",
    "for train_idx, test_idx in fold.split(x,y):\n",
    "    x_train=x[train_idx]\n",
    "    y_train=y[train_idx]\n",
    "    x_test=x[test_idx]\n",
    "    y_test=y[test_idx]\n",
    "\n",
    "    lgb_train=lgb.Dataset(x_train,y_train)\n",
    "    lgb_test=lgb.Dataset(x_test,y_test)\n",
    "    # 内側のCV（学習データのうち、ハイパーパラメータの学習データと評価データに分ける）\n",
    "    inner_fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1234)\n",
    "    tuner_cv = lgb.LightGBMTunerCV(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=1000,\n",
    "        folds=inner_fold,\n",
    "        callbacks=callbacks,\n",
    "        optuna_seed=1234,\n",
    "        return_cvbooster=True\n",
    "    )\n",
    "    tuner_cv.run()\n",
    "    # 最もよいモデルを取り出し、保存してからそのモデルをロードし、それでテストデータを予測する\n",
    "    best_model=tuner_cv.get_best_booster()\n",
    "    best_model.save_model(\"lgb_model.txt\")\n",
    "    best_model=lgb.Booster(model_file=\"lgb_model.txt\")\n",
    "    pred=best_model.predict(x_test)\n",
    "    pred=np.argmax(pred, axis=1)\n",
    "    \n",
    "    pred_all.append(pred)\n",
    "    true_all.append(y_test)\n",
    "    test_idx_all.append(test_idx)\n",
    "    print(confusion_matrix(pred,y_test))\n",
    "    print(accuracy_score(pred,y_test))\n",
    "\n",
    "time.time()-start_time\n",
    "\n",
    "pred_all_concat=np.concatenate(pred_all)\n",
    "true_all_concat=np.concatenate(true_all)\n",
    "test_idx_all_concat=np.concatenate(test_idx_all)\n",
    "df_pred=pd.DataFrame(zip(test_idx_all_concat,pred_all_concat),columns=[\"id\",\"pred\"]).sort_values(\"id\").assign(id=lambda d: d.id.astype(str))\n",
    "\n",
    "# 予測結果を結合したdf\n",
    "df_res=(\n",
    "    pd.merge(df,df_pred,on=\"id\",how=\"inner\")\n",
    "    .assign(pred=lambda d: le.inverse_transform(d.pred))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列と予測精度を学習したモデルから求めるならこれを使うが、\n",
    "# 本notebookでは事前に回したコードで求めている予測結果を読み込むためコメントアウト\n",
    "\n",
    "# conf_mat=pd.DataFrame(\n",
    "#     confusion_matrix(pred_all_concat,true_all_concat),\n",
    "#     # column,indexともに0-4なので\n",
    "#     columns=le.inverse_transform(np.unique(y)),\n",
    "#     index=le.inverse_transform(np.unique(y)),\n",
    "# )\n",
    "# print(conf_mat)\n",
    "# print(classification_report(df_res.author,df_res.pred)\n",
    "# print(\"accuracy_score: \" + str(accuracy_score(pred_all_concat,true_all_concat)))\n",
    "# print(df_res.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Waxy    和  小林ユリ  松はるな   美佳\n",
      "Waxy   650    7    14     2    7\n",
      "和        1  692     2    12   22\n",
      "小林ユリ    10   17   627    22    6\n",
      "松はるな     4   12     4  1518    6\n",
      "美佳       5   27     6    13  616\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Waxy       0.97      0.96      0.96       680\n",
      "           和       0.92      0.95      0.93       729\n",
      "        小林ユリ       0.96      0.92      0.94       682\n",
      "        松はるな       0.97      0.98      0.98      1544\n",
      "          美佳       0.94      0.92      0.93       667\n",
      "\n",
      "    accuracy                           0.95      4302\n",
      "   macro avg       0.95      0.95      0.95      4302\n",
      "weighted avg       0.95      0.95      0.95      4302\n",
      "\n",
      "accuracy_score: 0.9537424453742446\n"
     ]
    }
   ],
   "source": [
    "df_res=pd.read_csv(\"../googirl_classify_result.csv\")\n",
    "confusion_matrix(df_res.author,df_res.pred)\n",
    "\n",
    "conf_mat=pd.DataFrame(\n",
    "    confusion_matrix(df_res.author,df_res.pred),\n",
    "    # column,indexともに0-4なので\n",
    "    columns=le.inverse_transform(np.unique(y)),\n",
    "    index=le.inverse_transform(np.unique(y)),\n",
    ")\n",
    "print(conf_mat)\n",
    "print(classification_report(df_res.author,df_res.pred))\n",
    "print(\"accuracy_score: \" + str(accuracy_score(df_res.author,df_res.pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  id     id_url                                  url        date  \\\n",
      "0           0   0  210614009  https://googirl.jp/renai/210614009/  2021-06-14   \n",
      "1           1   1  210614006  https://googirl.jp/renai/210614006/  2021-06-14   \n",
      "2           2   2  210614019  https://googirl.jp/renai/210614019/  2021-06-14   \n",
      "3           3   3  210613006  https://googirl.jp/renai/210613006/  2021-06-13   \n",
      "4           4   4  210613015  https://googirl.jp/renai/210613015/  2021-06-13   \n",
      "\n",
      "                              title author  \\\n",
      "0              12星座別!彼の究極の欠点とは?【前編】     美佳   \n",
      "1    何が違うの?「彼女にしたい女」と「友達止まりの女」の3つの差   松はるな   \n",
      "2  直感って侮れない!「この人と結婚するかも…」が当たったケース4つ   Waxy   \n",
      "3     運命かも!男性が「相性のよさ」を感じて惚れる女性の言動3つ   松はるな   \n",
      "4   幼稚で付き合いきれない!「ナルシストだな…」と思う彼の言動4つ   Waxy   \n",
      "\n",
      "                                                text  pred  \n",
      "0  誰にでも一つくらいは欠点があるもの。付き合う上では、相手の欠点を受け入れられるかどうかがかな...    美佳  \n",
      "1  いい感じに仲よくなってきたな……と思いきや、「俺たちっていい友達だよな!」とまさかの友達発言...  松はるな  \n",
      "2  長いあいだ婚活を続けてもなかなかうまくいかない人もいれば、短期間でサクッと結婚を決めるような...  小林ユリ  \n",
      "3  「この人とは相性がいいかも!」と感じる相手とは関係が発展しやすいし、付き合うことになっても特...  松はるな  \n",
      "4  見た目は一人前の立派な大人の男性。でも中身は幼稚で、いつも周りから注目されていないと不満……...  Waxy  \n"
     ]
    }
   ],
   "source": [
    "print(df_res.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感想\n",
    "\n",
    "- 同じカテゴリの文章なので分類が難しいかと思いましたが、思いの外高い精度が出ました。\n",
    "  - 実はライターによって使っている単語が若干違います。それでも人間が読んで見分けるのは難しいです。\n",
    "- BoWは古典的な手法ですが、このようにタスクによっては強力です。\n",
    "- コラムに目を通して、このデータセットに適した前処理を考える所が一番大変でした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （参考）MeCabの出力のパースをする自作クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "class CustomizedTagger(MeCab.Tagger):\n",
    "    cols=[\"surface\",\"pos\",\"pos1\",\"pos2\",\"pos3\",\"form1\",\"form2\",\"base\",\"yomi\",\"hatsuon\"]\n",
    "\n",
    "    def parse(self,text: str) -> pd.DataFrame:\n",
    "        res=[]\n",
    "        node=self.parseToNode(text)\n",
    "        while node:\n",
    "            surface=[node.surface]\n",
    "            feature=node.feature.split(\",\")\n",
    "            if (feature[0]!=\"BOS/EOS\"):\n",
    "                res.append([*surface,*feature])\n",
    "            node=node.next\n",
    "        res_df=pd.DataFrame(res,columns=self.cols)\n",
    "        return res_df\n",
    "\n",
    "    # 分かち書きのうち、posに指定した品詞のみ原型にして取り出す\n",
    "    def wakati_base_extract(self,text: str,pos: List[str]) -> List[str]:\n",
    "        parsed=self.parse(text)\n",
    "        extracted_list=parsed.loc[lambda d: d.pos.isin(pos)].base.to_list()\n",
    "        res=\" \".join(extracted_list)\n",
    "        res=[res]\n",
    "        return res"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50983c823dc66f13afc4e1299b7b0860ccb10a5cf35faea61c989df5bae0bc3e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
