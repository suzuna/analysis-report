{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-WordsとLightGBMで恋愛コラムの本文からライターを判定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    "\n",
    "- [Googirl](https://googirl.jp/)という女性向け恋愛コラムのWebサイトがあります。\n",
    "- このサイトは、コラムの記事ごとにライターが異なります。コラムの本文からそのコラムを書いたライターを分類するLightGBMモデルを作ってみました。特徴量はBag-of-Words (BoW) です。\n",
    "  - コラムの総数が多い上位5人のライターに絞っています。\n",
    "    - つまり、記事のBoWを特徴量にした5クラス分類タスクです。\n",
    "  - なお、記事数は4000件程度、1本の記事は約500文字程度です。\n",
    "  - LightGBMはoptuna.integration.lightgbmを使いました。\n",
    "- 予測精度は96%程度でした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ\n",
    "\n",
    "- 対象は、Googirlの[恋愛・結婚カテゴリ](https://googirl.jp/renai/)のうち、2017/1/1～2021/6/15に書かれたコラムです。そのうち、投稿数が多い順に5名のライターのコラムのみを取り出しました。\n",
    "  - この5人で、全てのコラムの半分程度を占めます。\n",
    "  - 他にもカテゴリはありますが、同じカテゴリの文章を分類する方が難易度が高く面白いと思ったので、恋愛・結婚カテゴリのみに絞りました。\n",
    "  - ただし、続き物のコラムは最初の1本のみを取り出しました。\n",
    "    - vol.1～vol.9や第1位～第9位、前編・後編のように、同じライターが同じテーマを複数の記事に分割していることが多いです。この場合、vol.1、第1位、前編のみをデータに残し、それ以外の記事は含めていないということです。というのも、これらの記事は同じテーマであるため、ある意味ライターを予測できるのが当たり前だからです。\n",
    "  - 全部で4302件の記事です。\n",
    "- Googirlの恋愛・結婚カテゴリをスクレイピングし、タイトル、本文、ライター名を列に持つpd.DataFrameを事前に用意しています。\n",
    "  - 十分なスリープを挟みました。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | author   |    n |\n",
      "|---:|:---------|-----:|\n",
      "|  0 | 松はるな | 1544 |\n",
      "|  1 | 和       |  729 |\n",
      "|  2 | 小林ユリ |  682 |\n",
      "|  3 | Waxy     |  680 |\n",
      "|  4 | 美佳     |  667 |\n",
      "['松はるな', '和', '小林ユリ', 'Waxy', '美佳']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import neologdn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import optuna.integration.lightgbm as lgb\n",
    "from optuna.logging import set_verbosity\n",
    "from lightgbm import Dataset, early_stopping, log_evaluation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from customized_tagger import CustomizedTagger\n",
    "\n",
    "# 読み込みと前処理 ---------------------------------------------------------------------\n",
    "freq_author=5\n",
    "googirl_article=pd.read_csv(\"../data/googirl/googirl_article_renai_and_marriage_20210615.csv\")\n",
    "googirl_article=(\n",
    "    googirl_article\n",
    "    .assign(date=lambda d: pd.to_datetime(d.date))\n",
    "    .loc[lambda d: d.date>=datetime(2017,1,1)]\n",
    "    .assign(\n",
    "        title=lambda d: d.title.map(neologdn.normalize),\n",
    "    ).loc[\n",
    "        # [vV]ol. 1や前編、第1位以外の連番のタイトルの記事を削除する\n",
    "        (lambda d: \n",
    "            ~d.title.str.contains(r\"[vV]ol\\. ?[2-9]\") &\n",
    "            ~d.title.str.contains(\"[中後]編\") &\n",
    "            ~d.title.str.contains(\"第[2-9]位\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ライター別の記事数\n",
    "n_by_author=(\n",
    "    googirl_article.author\n",
    "    .value_counts(ascending=False)\n",
    "    # indexにあるauthorを列として戻すため\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\":\"author\",\"author\":\"n\"})\n",
    ")\n",
    "print(n_by_author.head(5).to_markdown())\n",
    "used_authors=n_by_author.author.head(freq_author).to_list()\n",
    "print(used_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "\n",
    "- 以下を削除しました。\n",
    "  - htmlタグ\n",
    "  - 改行\n",
    "  - 下三角形やかぎかっこの前の中黒\n",
    "    - ディバイダー的に使われているものです。\n",
    "  - インタビューした人の年齢など\n",
    "    - 削除しないとBoWを作る上でノイズになりえます。\n",
    "- 正規化はneologd.normalizeを用いました。\n",
    "  - NFKC正規化にいくつかの表記の揺れを直してくれる便利なライブラリです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_url</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>210614009</td>\n",
       "      <td>美佳</td>\n",
       "      <td>12星座別!彼の究極の欠点とは?【前編】</td>\n",
       "      <td>誰にでも一つくらいは欠点があるもの。付き合う上では、相手の欠点を受け入れられるかどうかがかな...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>210614006</td>\n",
       "      <td>松はるな</td>\n",
       "      <td>何が違うの?「彼女にしたい女」と「友達止まりの女」の3つの差</td>\n",
       "      <td>いい感じに仲よくなってきたな……と思いきや、「俺たちっていい友達だよな!」とまさかの友達発言...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>210614019</td>\n",
       "      <td>Waxy</td>\n",
       "      <td>直感って侮れない!「この人と結婚するかも…」が当たったケース4つ</td>\n",
       "      <td>長いあいだ婚活を続けてもなかなかうまくいかない人もいれば、短期間でサクッと結婚を決めるような...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>210613006</td>\n",
       "      <td>松はるな</td>\n",
       "      <td>運命かも!男性が「相性のよさ」を感じて惚れる女性の言動3つ</td>\n",
       "      <td>「この人とは相性がいいかも!」と感じる相手とは関係が発展しやすいし、付き合うことになっても特...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>210613015</td>\n",
       "      <td>Waxy</td>\n",
       "      <td>幼稚で付き合いきれない!「ナルシストだな…」と思う彼の言動4つ</td>\n",
       "      <td>見た目は一人前の立派な大人の男性。でも中身は幼稚で、いつも周りから注目されていないと不満……...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id     id_url author                             title  \\\n",
       "0  0  210614009     美佳              12星座別!彼の究極の欠点とは?【前編】   \n",
       "1  1  210614006   松はるな    何が違うの?「彼女にしたい女」と「友達止まりの女」の3つの差   \n",
       "2  2  210614019   Waxy  直感って侮れない!「この人と結婚するかも…」が当たったケース4つ   \n",
       "3  3  210613006   松はるな     運命かも!男性が「相性のよさ」を感じて惚れる女性の言動3つ   \n",
       "4  4  210613015   Waxy   幼稚で付き合いきれない!「ナルシストだな…」と思う彼の言動4つ   \n",
       "\n",
       "                                                text  \n",
       "0  誰にでも一つくらいは欠点があるもの。付き合う上では、相手の欠点を受け入れられるかどうかがかな...  \n",
       "1  いい感じに仲よくなってきたな……と思いきや、「俺たちっていい友達だよな!」とまさかの友達発言...  \n",
       "2  長いあいだ婚活を続けてもなかなかうまくいかない人もいれば、短期間でサクッと結婚を決めるような...  \n",
       "3  「この人とは相性がいいかも!」と感じる相手とは関係が発展しやすいし、付き合うことになっても特...  \n",
       "4  見た目は一人前の立派な大人の男性。でも中身は幼稚で、いつも周りから注目されていないと不満……...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article=(\n",
    "    googirl_article\n",
    "    .loc[lambda d: d.author.isin(used_authors)]\n",
    "    # urlの最後のサブディレクトリ（https://hoge.com/fuga/piyo/のpiyo）を取り出して一意なキーとして使う\n",
    "    .assign(id_url=lambda d: d.url.str.extract(\"([^/]+)(?=/$)\",expand=True))\n",
    "    # 連番を振る\n",
    "    .assign(id=lambda d: pd.RangeIndex(start=0,stop=len(d),step=1).astype(str))\n",
    "    .assign(\n",
    "        text=lambda d: d.text.str.replace(r\"[\\(（].*?[0-9]+[歳才代].*?[）\\)]\",\"\",regex=True)\n",
    "                                .str.replace(r\"\\n\",\"\",regex=True)\n",
    "                                .str.replace(r\"\\[\\[\\[p\\]\\]\\]\",\"\",regex=True)\n",
    "                                .str.replace(\"▽\",\"\",regex=True)\n",
    "                                .str.replace(\"・(?=「)\",\"\",regex=True)\n",
    "                                .map(neologdn.normalize)\n",
    "     )\n",
    "     .reset_index()\n",
    "     .filter([\"id\",\"id_url\",\"url\",\"date\",\"title\",\"author\",\"text\"])\n",
    ")\n",
    "\n",
    "df=article.filter([\"id\",\"id_url\",\"author\",\"title\",\"text\"])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析と特徴量のBoW作成\n",
    "\n",
    "- 環境\n",
    "  - Windows 10 + i9-9900K\n",
    "  - Python 3.10 + MeCab 32bit (v0.996) + [mecab](https://pypi.org/project/mecab/) (v.0.996.3) \n",
    "    - 辞書はデフォルトのIPA辞書と2020/8/20（本記事を執筆時で最新）のmecab-ipadic-NEologdです。\n",
    "    - MeCabのPythonバインディングはmecabを用いています。\n",
    "      - 私の環境ではmecab-python3のインストールに失敗したためです。\n",
    "- 特徴量\n",
    "  - 名詞のみ用いています。\n",
    "  - ストップワードに[Slothlib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)を用いています。\n",
    "  - BoWは2値にしています。\n",
    "  - scikit-learnのCountVectorizerのmin_dfを0.001にしています。\n",
    "    - つまり、4302記事の中の4記事以下でしか出現しない単語を削除しています。\n",
    "    - min_dfを0にすると12423個の特徴量（＝12423次元）ですが、0.001にしたことで4490個まで減りました。\n",
    "- CountVectorizerには、原型のまま分かち書きをして名詞のみ残したものを渡しています。\n",
    "  - 実装のコツとして、MeCab.Tagger(\"-Ochasen\");MeCab.Tagger.parseToNodeの結果をpd.DataFrameで返すラッパーの自作クラスを作っています（末尾のセルを参照）。このDataFrameから品詞が名詞のもののみを抽出して、その原型の空白を挟んでくっつけています。\n",
    "- MeCab.Tagger()は、MeCab.Tagger(\"-r .mecabrcのパス\")で使用する.mecabrcを指定できますので、UTF-8版の辞書を記載した.mecabrcを引数に与えることでUTF-8の辞書を使えるようにしています。\n",
    "  - 私はRとPythonの両方でMeCabを用いています。Windows環境でのMeCabは、RではShift-JIS、PythonではUTF-8の辞書を用いるため、デフォルトのipadicとNEologdともにShift-JISとUTF-8の両方の辞書を作成しています。Shift-JISの辞書とUTF-8の辞書それぞれを記載した.mecabrcを作っておくことで、RとPythonでMeCabを使い分けることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4302/4302 [00:10<00:00, 406.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# 形態素解析と特徴量を作る ---------------------------------------------------------------------\n",
    "ct=CustomizedTagger(\"-Ochasen -r ../utf8.mecabrc\")\n",
    "texts=df.text.to_list()\n",
    "# 形態素解析し、名詞のみを取り出して空白区切りの分かち書きにする\n",
    "# [np.array([\"吾輩 猫\"],np.array([\"名前\"])]のようなもの\n",
    "wakati=[np.array(ct.wakati_base_extract(j,[\"名詞\"])) for i,j in enumerate(tqdm(texts))]\n",
    "# np.array([\"吾輩 猫\",\"名前\"])にする\n",
    "wakati=np.concatenate(wakati)\n",
    "\n",
    "# Bag-of-Wordを作る\n",
    "stop_words=(\n",
    "    pd.read_csv(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\",header=None)\n",
    "    .set_axis([\"word\"],axis=1)\n",
    "    .word\n",
    "    .to_list()\n",
    ")\n",
    "cv=CountVectorizer(min_df=0.001,binary=True,stop_words=stop_words)\n",
    "cv_transform=cv.fit_transform(wakati)\n",
    "df_x=pd.DataFrame(cv_transform.toarray(),columns=cv.get_feature_names_out())\n",
    "x=np.array(df_x)\n",
    "\n",
    "# 目的変数のauthor列をラベルエンコーティングする\n",
    "le=LabelEncoder()\n",
    "df_y=df.assign(target=lambda d: le.fit_transform(d.author))\n",
    "y=np.array(df_y.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>100年</th>\n",
       "      <th>100点</th>\n",
       "      <th>10代</th>\n",
       "      <th>10分</th>\n",
       "      <th>10年</th>\n",
       "      <th>10歳</th>\n",
       "      <th>1163</th>\n",
       "      <th>12月</th>\n",
       "      <th>180度</th>\n",
       "      <th>...</th>\n",
       "      <th>魔法の言葉</th>\n",
       "      <th>鮮やか</th>\n",
       "      <th>鮮明</th>\n",
       "      <th>鳥肌</th>\n",
       "      <th>鵜呑み</th>\n",
       "      <th>鷲掴み</th>\n",
       "      <th>麻痺</th>\n",
       "      <th>黄色</th>\n",
       "      <th>黒歴史</th>\n",
       "      <th>黒髪</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4297</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4298</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4299</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4300</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4301</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4302 rows × 5467 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      100  100年  100点  10代  10分  10年  10歳  1163  12月  180度  ...  魔法の言葉  鮮やか  \\\n",
       "0       0     0     0    0    0    0    0     0    0     0  ...      0    0   \n",
       "1       0     0     0    0    0    0    0     0    0     0  ...      0    0   \n",
       "2       0     0     0    0    0    0    0     0    0     0  ...      0    0   \n",
       "3       0     0     0    0    0    0    0     0    0     0  ...      0    0   \n",
       "4       0     0     0    0    0    0    0     0    0     0  ...      0    0   \n",
       "...   ...   ...   ...  ...  ...  ...  ...   ...  ...   ...  ...    ...  ...   \n",
       "4297    0     0     0    0    0    0    0     0    0     0  ...      0    0   \n",
       "4298    0     0     0    0    0    0    0     0    0     0  ...      0    0   \n",
       "4299    0     0     0    0    0    0    0     0    0     0  ...      0    0   \n",
       "4300    0     0     0    0    0    0    0     0    0     0  ...      0    0   \n",
       "4301    0     0     0    0    0    0    0     0    0     0  ...      0    0   \n",
       "\n",
       "      鮮明  鳥肌  鵜呑み  鷲掴み  麻痺  黄色  黒歴史  黒髪  \n",
       "0      0   0    0    0   0   0    0   0  \n",
       "1      0   0    0    0   0   0    0   0  \n",
       "2      0   0    0    0   0   0    0   0  \n",
       "3      0   0    0    0   0   0    0   0  \n",
       "4      0   0    0    0   0   0    1   0  \n",
       "...   ..  ..  ...  ...  ..  ..  ...  ..  \n",
       "4297   0   0    0    0   0   0    0   0  \n",
       "4298   0   0    0    0   0   0    0   0  \n",
       "4299   0   0    0    0   0   0    0   0  \n",
       "4300   0   0    0    0   0   0    0   0  \n",
       "4301   0   0    0    0   0   0    0   0  \n",
       "\n",
       "[4302 rows x 5467 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       4\n",
       "1       3\n",
       "2       0\n",
       "3       3\n",
       "4       0\n",
       "       ..\n",
       "4297    0\n",
       "4298    0\n",
       "4299    0\n",
       "4300    3\n",
       "4301    2\n",
       "Name: target, Length: 4302, dtype: int32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習と評価\n",
    "\n",
    "- 環境\n",
    "  - scikit-learn: v1.0.2\n",
    "  - lightgbm: v3.3.2\n",
    "  - optuna: 2.10.0\n",
    "- LightGBMのライブラリはoptuna.integration.lightgbmを用いました。optunaのベイズ最適化でハイパーパラメータ探索を行いました。\n",
    "- 交差検証はNested CVです。\n",
    "  - 外側のCV（学習と予測のFold）と内側のCV（学習の中のハイパーパラメータの調整のFold）のいずれも、5-FoldのStratified Foldとしています。\n",
    "    - Stratifiedなのはラベルが不均衡だからです。\n",
    "- i9-9900Kの私の環境で1～2時間で終了したと思います。メモリは4GB程度使っていました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習と予測 ---------------------------------------------------------------------\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': freq_author,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'deterministic': True,\n",
    "    'force_row_wise': True\n",
    "    }\n",
    "set_verbosity(1)\n",
    "callbacks=[\n",
    "    early_stopping(stopping_rounds=20),\n",
    "    log_evaluation(period=20)\n",
    "]\n",
    "\n",
    "start_time=time.time()\n",
    "pred_all=[]\n",
    "true_all=[]\n",
    "test_idx_all=[]\n",
    "# 外側のCV（学習データと評価データ）\n",
    "fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1234)\n",
    "for train_idx, test_idx in fold.split(x,y):\n",
    "    x_train=x[train_idx]\n",
    "    y_train=y[train_idx]\n",
    "    x_test=x[test_idx]\n",
    "    y_test=y[test_idx]\n",
    "\n",
    "    lgb_train=lgb.Dataset(x_train,y_train)\n",
    "    lgb_test=lgb.Dataset(x_test,y_test)\n",
    "    # 内側のCV（学習データのうち、ハイパーパラメータの学習データと評価データに分ける）\n",
    "    inner_fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1234)\n",
    "    tuner_cv = lgb.LightGBMTunerCV(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=1000,\n",
    "        folds=inner_fold,\n",
    "        callbacks=callbacks,\n",
    "        optuna_seed=1234,\n",
    "        return_cvbooster=True\n",
    "    )\n",
    "    tuner_cv.run()\n",
    "    # 最もよいモデルを取り出し、保存してからそのモデルをロードし、それでテストデータを予測する\n",
    "    best_model=tuner_cv.get_best_booster()\n",
    "    best_model.save_model(\"lgb_model.txt\")\n",
    "    best_model=lgb.Booster(model_file=\"lgb_model.txt\")\n",
    "    pred=best_model.predict(x_test)\n",
    "    pred=np.argmax(pred, axis=1)\n",
    "    \n",
    "    pred_all.append(pred)\n",
    "    true_all.append(y_test)\n",
    "    test_idx_all.append(test_idx)\n",
    "    print(confusion_matrix(pred,y_test))\n",
    "    print(accuracy_score(pred,y_test))\n",
    "\n",
    "end_time=time.time()\n",
    "\n",
    "pred_all_concat=np.concatenate(pred_all)\n",
    "true_all_concat=np.concatenate(true_all)\n",
    "test_idx_all_concat=np.concatenate(test_idx_all)\n",
    "df_pred=pd.DataFrame(zip(test_idx_all_concat,pred_all_concat),columns=[\"id\",\"pred\"]).sort_values(\"id\").assign(id=lambda d: d.id.astype(str))\n",
    "\n",
    "# 予測結果を結合したdf\n",
    "df_res=(\n",
    "    pd.merge(df,df_pred,on=\"id\",how=\"inner\")\n",
    "    .assign(pred=lambda d: le.inverse_transform(d.pred))\n",
    ")\n",
    "df_res.to_csv(\"../googirl_classify_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測結果\n",
    "\n",
    "混同行列は行が正解のラベル、列が予測値のラベルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列と予測精度を学習したモデルから求めるならこれを使うが、\n",
    "# 本notebookでは事前に回したコードで求めている予測結果を読み込むためコメントアウト\n",
    "# conf_mat=pd.DataFrame(\n",
    "#     confusion_matrix(true_all_concat,pred_all_concat),\n",
    "#     # column,indexともに0-4なので\n",
    "#     columns=le.inverse_transform(np.unique(y)),\n",
    "#     index=le.inverse_transform(np.unique(y)),\n",
    "# )\n",
    "# print(conf_mat)\n",
    "# print(classification_report(df_res.author,df_res.pred,digits=5))\n",
    "# print(\"accuracy_score: \" + str(accuracy_score(pred_all_concat,true_all_concat)))\n",
    "# print(df_res.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Waxy    0.97598   0.95588   0.96582       680\n",
      "           和    0.95423   0.94376   0.94897       729\n",
      "        小林ユリ    0.95238   0.93842   0.94535       682\n",
      "        松はるな    0.96506   0.98381   0.97434      1544\n",
      "          美佳    0.95516   0.95802   0.95659       667\n",
      "\n",
      "    accuracy                        0.96141      4302\n",
      "   macro avg    0.96056   0.95598   0.95821      4302\n",
      "weighted avg    0.96140   0.96141   0.96135      4302\n",
      "\n",
      "accuracy_score: 0.9614132961413296\n"
     ]
    }
   ],
   "source": [
    "df_res=pd.read_csv(\"../googirl_classify_result.csv\")\n",
    "# 行がtrue, 列がpred\n",
    "conf_mat=pd.DataFrame(\n",
    "    confusion_matrix(df_res.author,df_res.pred),\n",
    "    # column,indexともに0-4なので\n",
    "    columns=le.inverse_transform(np.unique(y)),\n",
    "    index=le.inverse_transform(np.unique(y)),\n",
    ")\n",
    "print(classification_report(df_res.author,df_res.pred,digits=5))\n",
    "print(\"accuracy_score: \" + str(accuracy_score(df_res.author,df_res.pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Waxy</th>\n",
       "      <th>和</th>\n",
       "      <th>小林ユリ</th>\n",
       "      <th>松はるな</th>\n",
       "      <th>美佳</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Waxy</th>\n",
       "      <td>650</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>和</th>\n",
       "      <td>1</td>\n",
       "      <td>688</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>小林ユリ</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>640</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>松はるな</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>1519</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>美佳</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Waxy    和  小林ユリ  松はるな   美佳\n",
       "Waxy   650    8    14     3    5\n",
       "和        1  688     5    21   14\n",
       "小林ユリ     9    7   640    19    7\n",
       "松はるな     4    9     8  1519    4\n",
       "美佳       2    9     5    12  639"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_url</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>210614009</td>\n",
       "      <td>美佳</td>\n",
       "      <td>12星座別!彼の究極の欠点とは?【前編】</td>\n",
       "      <td>誰にでも一つくらいは欠点があるもの。付き合う上では、相手の欠点を受け入れられるかどうかがかな...</td>\n",
       "      <td>美佳</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>210614006</td>\n",
       "      <td>松はるな</td>\n",
       "      <td>何が違うの?「彼女にしたい女」と「友達止まりの女」の3つの差</td>\n",
       "      <td>いい感じに仲よくなってきたな……と思いきや、「俺たちっていい友達だよな!」とまさかの友達発言...</td>\n",
       "      <td>松はるな</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>210614019</td>\n",
       "      <td>Waxy</td>\n",
       "      <td>直感って侮れない!「この人と結婚するかも…」が当たったケース4つ</td>\n",
       "      <td>長いあいだ婚活を続けてもなかなかうまくいかない人もいれば、短期間でサクッと結婚を決めるような...</td>\n",
       "      <td>Waxy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>210613006</td>\n",
       "      <td>松はるな</td>\n",
       "      <td>運命かも!男性が「相性のよさ」を感じて惚れる女性の言動3つ</td>\n",
       "      <td>「この人とは相性がいいかも!」と感じる相手とは関係が発展しやすいし、付き合うことになっても特...</td>\n",
       "      <td>松はるな</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>210613015</td>\n",
       "      <td>Waxy</td>\n",
       "      <td>幼稚で付き合いきれない!「ナルシストだな…」と思う彼の言動4つ</td>\n",
       "      <td>見た目は一人前の立派な大人の男性。でも中身は幼稚で、いつも周りから注目されていないと不満……...</td>\n",
       "      <td>和</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     id_url author                             title  \\\n",
       "0   0  210614009     美佳              12星座別!彼の究極の欠点とは?【前編】   \n",
       "1   1  210614006   松はるな    何が違うの?「彼女にしたい女」と「友達止まりの女」の3つの差   \n",
       "2   2  210614019   Waxy  直感って侮れない!「この人と結婚するかも…」が当たったケース4つ   \n",
       "3   3  210613006   松はるな     運命かも!男性が「相性のよさ」を感じて惚れる女性の言動3つ   \n",
       "4   4  210613015   Waxy   幼稚で付き合いきれない!「ナルシストだな…」と思う彼の言動4つ   \n",
       "\n",
       "                                                text  pred  \n",
       "0  誰にでも一つくらいは欠点があるもの。付き合う上では、相手の欠点を受け入れられるかどうかがかな...    美佳  \n",
       "1  いい感じに仲よくなってきたな……と思いきや、「俺たちっていい友達だよな!」とまさかの友達発言...  松はるな  \n",
       "2  長いあいだ婚活を続けてもなかなかうまくいかない人もいれば、短期間でサクッと結婚を決めるような...  Waxy  \n",
       "3  「この人とは相性がいいかも!」と感じる相手とは関係が発展しやすいし、付き合うことになっても特...  松はるな  \n",
       "4  見た目は一人前の立派な大人の男性。でも中身は幼稚で、いつも周りから注目されていないと不満……...     和  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分類結果\n",
    "df_res.filter([\"id\",\"id_url\",\"author\",\"title\",\"text\",\"pred\"]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感想\n",
    "\n",
    "- 同じカテゴリの文章なので分類が難しいかと思いましたが、思いの外高い精度が出ました。\n",
    "  - ライターによって使っている単語が若干違うからだと思います。\n",
    "    - ライターごとにTF-IDFの高い単語を並べてみると、ライターによって使う単語の特徴が出ます。\n",
    "  - それでも人間が読んでライターを当てるのは難しいです。\n",
    "- BoWは古典的な手法ですが、このようにタスクによっては強力です。\n",
    "- 改行を削除するなど、前処理を丁寧に行うと精度が上がりました。\n",
    "- コラムに目を通してこのデータセットに適した前処理を考える所が一番大変でした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:11<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# ライター別にTF-IDFの上位100単語を並べる\n",
    "\n",
    "text_by_author=[\" \".join(df.loc[lambda d: d.author==i].text) for i in used_authors]\n",
    "wakati_by_author=np.concatenate(\n",
    "    [np.array(ct.wakati_base_extract(j,[\"名詞\"])) for i,j in enumerate(tqdm(text_by_author))]\n",
    ")\n",
    "\n",
    "tiv=TfidfVectorizer(max_features=1000,min_df=0.05,max_df=0.95,binary=False)\n",
    "# author x wordsのnp.array\n",
    "tiv_transform=tiv.fit_transform(wakati_by_author).toarray()\n",
    "\n",
    "feature_names = tiv.get_feature_names_out()\n",
    "index = tiv_transform.argsort(axis=1)[:,::-1]\n",
    "tfidf=(\n",
    "    pd.DataFrame([feature_names[x[:100]] for x in index])\n",
    "    .transpose()\n",
    "    .reset_index()\n",
    "    .set_axis([\"rank\"]+used_authors,axis=1)\n",
    "    .assign(rank=lambda d: (d[\"rank\"]+1).astype(str) + \"位\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （参考）MeCabの出力のパースをする自作クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "\n",
    "class CustomizedTagger(MeCab.Tagger):\n",
    "    cols=[\"surface\",\"pos\",\"pos1\",\"pos2\",\"pos3\",\"form1\",\"form2\",\"base\",\"yomi\",\"hatsuon\"]\n",
    "\n",
    "    # MeCab.Tagger.parseToNodeをpd.DataFrameで返す\n",
    "    def parse(self,text: str) -> pd.DataFrame:\n",
    "        res=[]\n",
    "        node=self.parseToNode(text)\n",
    "        while node:\n",
    "            surface=[node.surface]\n",
    "            feature=node.feature.split(\",\")\n",
    "            if (feature[0]!=\"BOS/EOS\"):\n",
    "                res.append([*surface,*feature])\n",
    "            node=node.next\n",
    "        res_df=pd.DataFrame(res,columns=self.cols)\n",
    "        return res_df\n",
    "\n",
    "    # 原型で分かち書きし、posに指定した品詞のみ取り出す\n",
    "    def wakati_base_extract(self,text: str,pos: list[str]) -> list[str]:\n",
    "        parsed=self.parse(text)\n",
    "        extracted_list=parsed.loc[lambda d: d.pos.isin(pos)].base.to_list()\n",
    "        res=\" \".join(extracted_list)\n",
    "        res=[res]\n",
    "        return res"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50983c823dc66f13afc4e1299b7b0860ccb10a5cf35faea61c989df5bae0bc3e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
