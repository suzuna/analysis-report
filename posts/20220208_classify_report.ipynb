{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-WordsとLightGBMで恋愛コラムの本文からライターを判定する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概要\n",
    "\n",
    "- [Googirl](https://googirl.jp/)という女性向け恋愛コラムのWebサイトがあります。\n",
    "- このサイトは、コラムの記事ごとにライターが異なります。コラムの本文からそのコラムを書いたライターを分類するLightGBMモデルを作ってみました。特徴量はBag-of-Words (BoW) です。\n",
    "  - コラムの総数が多い上位10人のライターに絞っています。\n",
    "    - つまり、記事のBoWを特徴量にした5クラス分類タスクです。\n",
    "  - なお、記事数は7000件程度、1本の記事は1000文字程度です。\n",
    "  - LightGBMはoptuna.integration.lightgbmを使いました。\n",
    "- 予測精度は92%程度でした。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ\n",
    "\n",
    "- 対象は、Googirlの[恋愛・結婚カテゴリ](https://googirl.jp/renai/)のうち、2017/1/1～2021/6/15に書かれたコラムです。そのうち、投稿数が多い順に10人のライターのコラムのみを取り出しました。\n",
    "  - この10人で、全てのコラムの7割程度を占めます。\n",
    "  - 他にもカテゴリはありますが、同じカテゴリの文章を分類する方が難易度が高く面白いと思ったので、恋愛・結婚カテゴリのみに絞りました。\n",
    "  - ただし、続き物のコラムは最初の1本のみを取り出しました。\n",
    "    - vol.1～vol.9や第1位～第9位、前編・後編のように、同じライターが同じテーマを複数の記事に分割していることが多いです。この場合、vol.1、第1位、前編のみをデータに残し、それ以外の記事は含めていないということです。というのも、これらの記事は同じテーマであるため、ある意味ライターを予測できるのが当たり前だからです。\n",
    "  - 全部で6955件の記事です。\n",
    "- Googirlの恋愛・結婚カテゴリをスクレイピングし、タイトル、本文、ライター名を列に持つpd.DataFrameを事前に用意しています。\n",
    "  - 十分なスリープを挟みました。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    | author       |    n |\n",
      "|---:|:-------------|-----:|\n",
      "|  0 | 松はるな     | 1544 |\n",
      "|  1 | 和           |  729 |\n",
      "|  2 | 小林ユリ     |  682 |\n",
      "|  3 | Waxy         |  677 |\n",
      "|  4 | 美佳         |  667 |\n",
      "|  5 | 占い師シータ |  665 |\n",
      "|  6 | きいろ       |  587 |\n",
      "|  7 | チオリーヌ   |  524 |\n",
      "|  8 | 小林リズム   |  497 |\n",
      "|  9 | 原桃子       |  383 |\n",
      "['松はるな', '和', '小林ユリ', 'Waxy', '美佳', '占い師シータ', 'きいろ', 'チオリーヌ', '小林リズム', '原桃子']\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import neologdn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import optuna.integration.lightgbm as lgb\n",
    "from optuna.logging import set_verbosity\n",
    "from lightgbm import Dataset, early_stopping, log_evaluation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from customized_tagger import CustomizedTagger\n",
    "\n",
    "# 読み込みと前処理 ---------------------------------------------------------------------\n",
    "freq_author=10\n",
    "googirl_article=pd.read_csv(\"../data/googirl/googirl_article_renai_and_marriage_20210615.csv\")\n",
    "googirl_article=(\n",
    "    googirl_article\n",
    "    .assign(date=lambda d: pd.to_datetime(d.date))\n",
    "    .loc[lambda d: d.date>=datetime(2017,1,1)]\n",
    "    .assign(\n",
    "        title=lambda d: d.title.map(neologdn.normalize),\n",
    "    ).loc[\n",
    "        # [vV]ol. 1や前編、第1位以外の連番のタイトルの記事を削除する\n",
    "        (lambda d: \n",
    "            ~d.title.str.contains(r\"[vV]ol\\. ?[2-9]\") &\n",
    "            ~d.title.str.contains(\"[中後]編\") &\n",
    "            ~d.title.str.contains(\"第[2-9]位\")\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ライター別の記事数\n",
    "n_by_author=(\n",
    "    googirl_article.author\n",
    "    .value_counts(ascending=False)\n",
    "    # indexにあるauthorを列として戻すため\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\":\"author\",\"author\":\"n\"})\n",
    ")\n",
    "print(n_by_author.head(freq_author).to_markdown())\n",
    "used_authors=n_by_author.author.head(freq_author).to_list()\n",
    "print(used_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "\n",
    "- 以下を削除しました。\n",
    "  - htmlタグ\n",
    "  - 改行\n",
    "  - 下三角形やかぎかっこの前の中黒\n",
    "    - ディバイダー的に使われているものです。\n",
    "  - インタビューした人の年齢など\n",
    "    - 削除しないとBoWを作る上でノイズになりえます。\n",
    "- 正規化はneologd.normalizeを用いました。\n",
    "  - NFKC正規化にいくつかの表記の揺れを直してくれる便利なライブラリです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_url</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>210615020</td>\n",
       "      <td>きいろ</td>\n",
       "      <td>こういう言葉がほしかった!もらうとうれしい恋愛アドバイス4選</td>\n",
       "      <td>恋のアドバイスといっても、どんな言葉をかけたらいいのか悩みますよね。「もっといい人がいるよ!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>210614016</td>\n",
       "      <td>原桃子</td>\n",
       "      <td>性格悪すぎてひどい…!圧倒的にモテない女子の特徴TOP7【第1位】</td>\n",
       "      <td>可愛くてオシャレなのに、なぜかいつまでたっても彼氏ができない女子っていますよね。そんな女子た...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>210614007</td>\n",
       "      <td>小林リズム</td>\n",
       "      <td>「もっといい人がいるはず」と別れを決意した女性の“その後\"とは?</td>\n",
       "      <td>結婚に向いていそうな人と付き合ったものの、心のどこかで納得できておらず……。「もっといい人が...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>210614009</td>\n",
       "      <td>美佳</td>\n",
       "      <td>12星座別!彼の究極の欠点とは?【前編】</td>\n",
       "      <td>誰にでも一つくらいは欠点があるもの。付き合う上では、相手の欠点を受け入れられるかどうかがかな...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>210614006</td>\n",
       "      <td>松はるな</td>\n",
       "      <td>何が違うの?「彼女にしたい女」と「友達止まりの女」の3つの差</td>\n",
       "      <td>いい感じに仲よくなってきたな……と思いきや、「俺たちっていい友達だよな!」とまさかの友達発言...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id     id_url author                              title  \\\n",
       "0  0  210615020    きいろ     こういう言葉がほしかった!もらうとうれしい恋愛アドバイス4選   \n",
       "1  1  210614016    原桃子  性格悪すぎてひどい…!圧倒的にモテない女子の特徴TOP7【第1位】   \n",
       "2  2  210614007  小林リズム   「もっといい人がいるはず」と別れを決意した女性の“その後\"とは?   \n",
       "3  3  210614009     美佳               12星座別!彼の究極の欠点とは?【前編】   \n",
       "4  4  210614006   松はるな     何が違うの?「彼女にしたい女」と「友達止まりの女」の3つの差   \n",
       "\n",
       "                                                text  \n",
       "0  恋のアドバイスといっても、どんな言葉をかけたらいいのか悩みますよね。「もっといい人がいるよ!...  \n",
       "1  可愛くてオシャレなのに、なぜかいつまでたっても彼氏ができない女子っていますよね。そんな女子た...  \n",
       "2  結婚に向いていそうな人と付き合ったものの、心のどこかで納得できておらず……。「もっといい人が...  \n",
       "3  誰にでも一つくらいは欠点があるもの。付き合う上では、相手の欠点を受け入れられるかどうかがかな...  \n",
       "4  いい感じに仲よくなってきたな……と思いきや、「俺たちっていい友達だよな!」とまさかの友達発言...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article=(\n",
    "    googirl_article\n",
    "    .loc[lambda d: d.author.isin(used_authors)]\n",
    "    # urlの最後のサブディレクトリ（https://hoge.com/fuga/piyo/のpiyo）を取り出して一意なキーとして使う\n",
    "    .assign(id_url=lambda d: d.url.str.extract(\"([^/]+)(?=/$)\",expand=True))\n",
    "    # 連番を振る\n",
    "    .assign(id=lambda d: pd.RangeIndex(start=0,stop=len(d),step=1).astype(str))\n",
    "    .assign(\n",
    "        text=lambda d: d.text.str.replace(r\"[\\(（].*?[0-9]+[歳才代].*?[）\\)]\",\"\",regex=True)\n",
    "                                .str.replace(r\"\\n\",\"\",regex=True)\n",
    "                                .str.replace(r\"\\[\\[\\[p\\]\\]\\]\",\"\",regex=True)\n",
    "                                .str.replace(\"▽\",\"\",regex=True)\n",
    "                                .str.replace(\"・(?= ?「)\",\"\",regex=True)\n",
    "                                .map(neologdn.normalize)\n",
    "     )\n",
    "     .reset_index()\n",
    "     .filter([\"id\",\"id_url\",\"url\",\"date\",\"title\",\"author\",\"text\"])\n",
    ")\n",
    "\n",
    "df=article.filter([\"id\",\"id_url\",\"author\",\"title\",\"text\"])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 形態素解析と特徴量のBoW作成\n",
    "\n",
    "- 環境\n",
    "  - Windows 10 + i9-9900K\n",
    "  - Python 3.10 + MeCab 32bit (v0.996) + [mecab](https://pypi.org/project/mecab/) (v.0.996.3) \n",
    "    - 辞書はデフォルトのIPA辞書と2020/8/20（本記事を執筆時で最新）のmecab-ipadic-NEologdです。\n",
    "    - MeCabのPythonバインディングはmecabを用いています。\n",
    "      - 私の環境ではmecab-python3のインストールに失敗したためです。\n",
    "- 特徴量\n",
    "  - 名詞のみ用いています。\n",
    "  - ストップワードに[Slothlib](http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt)を用いています。\n",
    "  - BoWは2値にしています。\n",
    "  - scikit-learnのCountVectorizerのmin_dfを0.001にしています。\n",
    "    - つまり、6955記事の中の6記事以下でしか出現しない単語を削除しています。\n",
    "    - min_dfを0にすると17255個の特徴量（＝17255次元）ですが、0.001にしたことで5772個まで減りました。\n",
    "- CountVectorizerには、原型のまま分かち書きをして名詞のみ残したものを渡しています。\n",
    "  - 実装のコツとして、MeCab.Tagger(\"-Ochasen\");MeCab.Tagger.parseToNodeの結果をpd.DataFrameで返すラッパーの自作クラスを作っています（末尾のセルを参照）。このDataFrameから品詞が名詞のもののみを抽出して、その原型の空白を挟んでくっつけています。\n",
    "- MeCab.Tagger()は、MeCab.Tagger(\"-r .mecabrcのパス\")で使用する.mecabrcを指定できますので、UTF-8版の辞書を記載した.mecabrcを引数に与えることでUTF-8の辞書を使えるようにしています。\n",
    "  - 私はRとPythonの両方でMeCabを用いています。Windows環境でのMeCabは、RではShift-JIS、PythonではUTF-8の辞書を用いるため、デフォルトのipadicとNEologdともにShift-JISとUTF-8の両方の辞書を作成しています。Shift-JISの辞書とUTF-8の辞書それぞれを記載した.mecabrcを作っておくことで、RとPythonでMeCabを使い分けることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6955/6955 [00:17<00:00, 403.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 形態素解析と特徴量を作る ---------------------------------------------------------------------\n",
    "ct=CustomizedTagger(\"-Ochasen -r ../utf8.mecabrc\")\n",
    "texts=df.text.to_list()\n",
    "# 形態素解析し、名詞のみを取り出して空白区切りの分かち書きにする\n",
    "# [np.array([\"吾輩 猫\"],np.array([\"名前\"])]のようなもの\n",
    "wakati=[np.array(ct.wakati_base_extract(j,[\"名詞\"])) for i,j in enumerate(tqdm(texts))]\n",
    "# np.array([\"吾輩 猫\",\"名前\"])にする\n",
    "wakati=np.concatenate(wakati)\n",
    "\n",
    "# Bag-of-Wordを作る\n",
    "stop_words=(\n",
    "    pd.read_csv(\"http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt\",header=None)\n",
    "    .set_axis([\"word\"],axis=1)\n",
    "    .word\n",
    "    .to_list()\n",
    ")\n",
    "cv=CountVectorizer(min_df=0.001,binary=True,stop_words=stop_words)\n",
    "cv_transform=cv.fit_transform(wakati)\n",
    "df_x=pd.DataFrame(cv_transform.toarray(),columns=cv.get_feature_names_out())\n",
    "x=np.array(df_x)\n",
    "\n",
    "# 目的変数のauthor列をラベルエンコーティングする\n",
    "le=LabelEncoder()\n",
    "df_y=df.assign(target=lambda d: le.fit_transform(d.author))\n",
    "y=np.array(df_y.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>100</th>\n",
       "      <th>100倍</th>\n",
       "      <th>100年</th>\n",
       "      <th>100点</th>\n",
       "      <th>10万円</th>\n",
       "      <th>10人</th>\n",
       "      <th>10代</th>\n",
       "      <th>10分</th>\n",
       "      <th>10年</th>\n",
       "      <th>10歳</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6951</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6952</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6953</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6954</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6955 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      100  100倍  100年  100点  10万円  10人  10代  10分  10年  10歳\n",
       "0       0     0     0     0     0    0    0    0    0    0\n",
       "1       0     0     0     0     0    0    0    0    0    0\n",
       "2       0     0     0     0     0    0    0    0    0    0\n",
       "3       0     0     0     0     0    0    0    0    0    0\n",
       "4       0     0     0     0     0    0    0    0    0    0\n",
       "...   ...   ...   ...   ...   ...  ...  ...  ...  ...  ...\n",
       "6950    0     0     0     0     0    0    0    0    0    0\n",
       "6951    0     0     0     0     0    0    0    0    0    0\n",
       "6952    0     0     0     0     0    0    0    0    0    0\n",
       "6953    0     0     0     0     0    0    0    0    0    0\n",
       "6954    0     0     0     0     0    0    0    0    0    0\n",
       "\n",
       "[6955 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_x.iloc[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       4\n",
       "2       7\n",
       "3       9\n",
       "4       8\n",
       "       ..\n",
       "6950    1\n",
       "6951    1\n",
       "6952    2\n",
       "6953    8\n",
       "6954    6\n",
       "Name: target, Length: 6955, dtype: int32"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習と評価\n",
    "\n",
    "- 環境\n",
    "  - scikit-learn: v1.0.2\n",
    "  - lightgbm: v3.3.2\n",
    "  - optuna: v2.10.0\n",
    "- LightGBMのライブラリはoptuna.integration.lightgbmを用いました。optunaのベイズ最適化でハイパーパラメータ探索を行いました。\n",
    "- 交差検証はNested CVです。\n",
    "  - 外側のCV（学習と予測のFold）と内側のCV（学習の中のハイパーパラメータの調整のFold）のいずれも、5-FoldのStratified Foldとしています。\n",
    "    - Stratifiedなのはラベルが不均衡だからです。\n",
    "- i9-9900Kの私の環境で3～4時間で終了したと思います。メモリは3GB程度使っていました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習と予測 ---------------------------------------------------------------------\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': freq_author,\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'deterministic': True,\n",
    "    'force_row_wise': True\n",
    "    }\n",
    "set_verbosity(1)\n",
    "callbacks=[\n",
    "    early_stopping(stopping_rounds=20),\n",
    "    log_evaluation(period=20)\n",
    "]\n",
    "\n",
    "start_time=time.time()\n",
    "pred_all=[]\n",
    "true_all=[]\n",
    "test_idx_all=[]\n",
    "# 外側のCV（学習データと評価データ）\n",
    "fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1234)\n",
    "for train_idx, test_idx in fold.split(x,y):\n",
    "    x_train=x[train_idx]\n",
    "    y_train=y[train_idx]\n",
    "    x_test=x[test_idx]\n",
    "    y_test=y[test_idx]\n",
    "\n",
    "    lgb_train=lgb.Dataset(x_train,y_train)\n",
    "    lgb_test=lgb.Dataset(x_test,y_test)\n",
    "    # 内側のCV（学習データのうち、ハイパーパラメータの学習データと評価データに分ける）\n",
    "    inner_fold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1234)\n",
    "    tuner_cv = lgb.LightGBMTunerCV(\n",
    "        params,\n",
    "        lgb_train,\n",
    "        num_boost_round=1000,\n",
    "        folds=inner_fold,\n",
    "        callbacks=callbacks,\n",
    "        optuna_seed=1234,\n",
    "        return_cvbooster=True\n",
    "    )\n",
    "    tuner_cv.run()\n",
    "    # 最もよいモデルを取り出し、保存してからそのモデルをロードし、それでテストデータを予測する\n",
    "    best_model=tuner_cv.get_best_booster()\n",
    "    best_model.save_model(\"lgb_model.txt\")\n",
    "    best_model=lgb.Booster(model_file=\"lgb_model.txt\")\n",
    "    pred=best_model.predict(x_test)\n",
    "    pred=np.argmax(pred, axis=1)\n",
    "    \n",
    "    pred_all.append(pred)\n",
    "    true_all.append(y_test)\n",
    "    test_idx_all.append(test_idx)\n",
    "    print(confusion_matrix(pred,y_test))\n",
    "    print(accuracy_score(pred,y_test))\n",
    "\n",
    "end_time=time.time()\n",
    "\n",
    "pred_all_concat=np.concatenate(pred_all)\n",
    "true_all_concat=np.concatenate(true_all)\n",
    "test_idx_all_concat=np.concatenate(test_idx_all)\n",
    "df_pred=pd.DataFrame(zip(test_idx_all_concat,pred_all_concat),columns=[\"id\",\"pred\"]).sort_values(\"id\").assign(id=lambda d: d.id.astype(str))\n",
    "\n",
    "# 予測結果を結合したdf\n",
    "df_res=(\n",
    "    pd.merge(df,df_pred,on=\"id\",how=\"inner\")\n",
    "    .assign(pred=lambda d: le.inverse_transform(d.pred))\n",
    ")\n",
    "df_res.to_csv(\"../googirl_classify_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測結果\n",
    "\n",
    "混同行列は行が正解のラベル、列が予測値のラベルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 混同行列と予測精度を学習したモデルから求めるならこれを使うが、\n",
    "# 本notebookでは事前に回したコードで求めている予測結果を読み込むためコメントアウト\n",
    "# conf_mat=pd.DataFrame(\n",
    "#     confusion_matrix(true_all_concat,pred_all_concat),\n",
    "#     # column,indexともに0-9なので\n",
    "#     columns=le.inverse_transform(np.unique(y)),\n",
    "#     index=le.inverse_transform(np.unique(y)),\n",
    "# )\n",
    "# print(conf_mat)\n",
    "# print(classification_report(df_res.author,df_res.pred,digits=5))\n",
    "# print(\"accuracy_score: \" + str(accuracy_score(pred_all_concat,true_all_concat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Waxy    0.95067   0.93944   0.94502       677\n",
      "         きいろ    0.83956   0.91823   0.87714       587\n",
      "       チオリーヌ    0.93933   0.91603   0.92754       524\n",
      "      占い師シータ    0.95542   0.96692   0.96114       665\n",
      "         原桃子    0.89118   0.79112   0.83817       383\n",
      "           和    0.93046   0.91770   0.92403       729\n",
      "        小林ユリ    0.92377   0.90616   0.91488       682\n",
      "       小林リズム    0.90020   0.88934   0.89474       497\n",
      "        松はるな    0.94997   0.97150   0.96061      1544\n",
      "          美佳    0.94260   0.93553   0.93905       667\n",
      "\n",
      "    accuracy                        0.92797      6955\n",
      "   macro avg    0.92232   0.91520   0.91823      6955\n",
      "weighted avg    0.92832   0.92797   0.92777      6955\n",
      "\n",
      "accuracy_score: 0.9279654924514738\n"
     ]
    }
   ],
   "source": [
    "df_res=pd.read_csv(\"../googirl_classify_result.csv\")\n",
    "# 行がtrue, 列がpred\n",
    "conf_mat=pd.DataFrame(\n",
    "    confusion_matrix(df_res.author,df_res.pred),\n",
    "    # column,indexともに0-9なので\n",
    "    columns=le.inverse_transform(np.unique(y)),\n",
    "    index=le.inverse_transform(np.unique(y)),\n",
    ")\n",
    "print(classification_report(df_res.author,df_res.pred,digits=5))\n",
    "print(\"accuracy_score: \" + str(accuracy_score(df_res.author,df_res.pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Waxy</th>\n",
       "      <th>きいろ</th>\n",
       "      <th>チオリーヌ</th>\n",
       "      <th>占い師シータ</th>\n",
       "      <th>原桃子</th>\n",
       "      <th>和</th>\n",
       "      <th>小林ユリ</th>\n",
       "      <th>小林リズム</th>\n",
       "      <th>松はるな</th>\n",
       "      <th>美佳</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Waxy</th>\n",
       "      <td>636</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>きいろ</th>\n",
       "      <td>4</td>\n",
       "      <td>539</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>チオリーヌ</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>480</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>占い師シータ</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>643</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>原桃子</th>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>303</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>和</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>669</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>小林ユリ</th>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>618</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>小林リズム</th>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>442</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>松はるな</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1500</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>美佳</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Waxy  きいろ  チオリーヌ  占い師シータ  原桃子    和  小林ユリ  小林リズム  松はるな   美佳\n",
       "Waxy     636    8      4       4    0    3    14      2     1    5\n",
       "きいろ        4  539      5       4    3    7     8      4    10    3\n",
       "チオリーヌ      4    9    480       1    5    3     2     16     1    3\n",
       "占い師シータ     1    8      1     643    0    2     4      4     2    0\n",
       "原桃子        7   13      5       2  303   14     4      7    18   10\n",
       "和          2   12      0       5    9  669     4      2    14   12\n",
       "小林ユリ       6   16      3       4    8    5   618      6    15    1\n",
       "小林リズム      3   15      9       6    2    5     6    442     8    1\n",
       "松はるな       4   13      2       3    3    5     5      6  1500    3\n",
       "美佳         2    9      2       1    7    6     4      2    10  624"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>id_url</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>210615020</td>\n",
       "      <td>きいろ</td>\n",
       "      <td>こういう言葉がほしかった!もらうとうれしい恋愛アドバイス4選</td>\n",
       "      <td>恋のアドバイスといっても、どんな言葉をかけたらいいのか悩みますよね。「もっといい人がいるよ!...</td>\n",
       "      <td>きいろ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>210614016</td>\n",
       "      <td>原桃子</td>\n",
       "      <td>性格悪すぎてひどい…!圧倒的にモテない女子の特徴TOP7【第1位】</td>\n",
       "      <td>可愛くてオシャレなのに、なぜかいつまでたっても彼氏ができない女子っていますよね。そんな女子た...</td>\n",
       "      <td>原桃子</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>210614007</td>\n",
       "      <td>小林リズム</td>\n",
       "      <td>「もっといい人がいるはず」と別れを決意した女性の“その後\"とは?</td>\n",
       "      <td>結婚に向いていそうな人と付き合ったものの、心のどこかで納得できておらず……。「もっといい人が...</td>\n",
       "      <td>小林リズム</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>210614009</td>\n",
       "      <td>美佳</td>\n",
       "      <td>12星座別!彼の究極の欠点とは?【前編】</td>\n",
       "      <td>誰にでも一つくらいは欠点があるもの。付き合う上では、相手の欠点を受け入れられるかどうかがかな...</td>\n",
       "      <td>美佳</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>210614006</td>\n",
       "      <td>松はるな</td>\n",
       "      <td>何が違うの?「彼女にしたい女」と「友達止まりの女」の3つの差</td>\n",
       "      <td>いい感じに仲よくなってきたな……と思いきや、「俺たちっていい友達だよな!」とまさかの友達発言...</td>\n",
       "      <td>松はるな</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id     id_url author                              title  \\\n",
       "0   0  210615020    きいろ     こういう言葉がほしかった!もらうとうれしい恋愛アドバイス4選   \n",
       "1   1  210614016    原桃子  性格悪すぎてひどい…!圧倒的にモテない女子の特徴TOP7【第1位】   \n",
       "2   2  210614007  小林リズム   「もっといい人がいるはず」と別れを決意した女性の“その後\"とは?   \n",
       "3   3  210614009     美佳               12星座別!彼の究極の欠点とは?【前編】   \n",
       "4   4  210614006   松はるな     何が違うの?「彼女にしたい女」と「友達止まりの女」の3つの差   \n",
       "\n",
       "                                                text   pred  \n",
       "0  恋のアドバイスといっても、どんな言葉をかけたらいいのか悩みますよね。「もっといい人がいるよ!...    きいろ  \n",
       "1  可愛くてオシャレなのに、なぜかいつまでたっても彼氏ができない女子っていますよね。そんな女子た...    原桃子  \n",
       "2  結婚に向いていそうな人と付き合ったものの、心のどこかで納得できておらず……。「もっといい人が...  小林リズム  \n",
       "3  誰にでも一つくらいは欠点があるもの。付き合う上では、相手の欠点を受け入れられるかどうかがかな...     美佳  \n",
       "4  いい感じに仲よくなってきたな……と思いきや、「俺たちっていい友達だよな!」とまさかの友達発言...   松はるな  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 分類結果\n",
    "df_res.filter([\"id\",\"id_url\",\"author\",\"title\",\"text\",\"pred\"]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感想\n",
    "\n",
    "- 同じカテゴリの文章なので分類が難しいかと思いましたが、思いの外高い精度が出ました。\n",
    "  - ライターによって使っている単語が若干違うからだと思います。\n",
    "    - ライターごとにTF-IDFの高い単語を並べてみると、ライターによって使う単語の特徴が出ます。\n",
    "  - それでも人間が読んでライターを当てるのは難しいです。\n",
    "- BoWは古典的な手法ですが、このようにタスクによっては強力です。\n",
    "- 改行を削除するなど、前処理を丁寧に行うと精度が上がりました。\n",
    "- コラムに目を通してこのデータセットに適した前処理を考える所と、想定通り前処理できているか確認して都度正規表現を書く所が一番大変でした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# ライター別にTF-IDFの上位100単語を並べる\n",
    "\n",
    "text_by_author=[\" \".join(df.loc[lambda d: d.author==i].text) for i in used_authors]\n",
    "wakati_by_author=np.concatenate(\n",
    "    [np.array(ct.wakati_base_extract(j,[\"名詞\"])) for i,j in enumerate(tqdm(text_by_author))]\n",
    ")\n",
    "\n",
    "tiv=TfidfVectorizer(max_features=1000,min_df=0.05,max_df=0.95,binary=False)\n",
    "# author x wordsのnp.array\n",
    "tiv_transform=tiv.fit_transform(wakati_by_author).toarray()\n",
    "\n",
    "feature_names = tiv.get_feature_names_out()\n",
    "index = tiv_transform.argsort(axis=1)[:,::-1]\n",
    "tfidf=(\n",
    "    pd.DataFrame([feature_names[x[:100]] for x in index])\n",
    "    .transpose()\n",
    "    .reset_index()\n",
    "    .set_axis([\"rank\"]+used_authors,axis=1)\n",
    "    .assign(rank=lambda d: (d[\"rank\"]+1).astype(str) + \"位\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## （参考）MeCabの出力のパースをする自作クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "\n",
    "class CustomizedTagger(MeCab.Tagger):\n",
    "    cols=[\"surface\",\"pos\",\"pos1\",\"pos2\",\"pos3\",\"form1\",\"form2\",\"base\",\"yomi\",\"hatsuon\"]\n",
    "\n",
    "    # MeCab.Tagger.parseToNodeをpd.DataFrameで返す\n",
    "    def parse(self,text: str) -> pd.DataFrame:\n",
    "        res=[]\n",
    "        node=self.parseToNode(text)\n",
    "        while node:\n",
    "            surface=[node.surface]\n",
    "            feature=node.feature.split(\",\")\n",
    "            if (feature[0]!=\"BOS/EOS\"):\n",
    "                res.append([*surface,*feature])\n",
    "            node=node.next\n",
    "        res_df=pd.DataFrame(res,columns=self.cols)\n",
    "        return res_df\n",
    "\n",
    "    # 原型で分かち書きし、posに指定した品詞のみ取り出す\n",
    "    def wakati_base_extract(self,text: str,pos: list[str]) -> list[str]:\n",
    "        parsed=self.parse(text)\n",
    "        extracted_list=parsed.loc[lambda d: d.pos.isin(pos)].base.to_list()\n",
    "        res=\" \".join(extracted_list)\n",
    "        res=[res]\n",
    "        return res"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "50983c823dc66f13afc4e1299b7b0860ccb10a5cf35faea61c989df5bae0bc3e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
